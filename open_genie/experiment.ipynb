{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc010fbd-ebf4-4dfb-8c1e-1a344d44fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genie import LatentAction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3806aa-01a6-41fa-aaf4-de0ff47a9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from math import prod\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from genie.module import parse_blueprint\n",
    "from genie.module.quantization import LookupFreeQuantization\n",
    "from genie.module.video import CausalConv3d, Downsample, Upsample\n",
    "from genie.utils import Blueprint\n",
    "\n",
    "REPR_ACT_ENC = (\n",
    "    ('space-time_attn', {\n",
    "        'n_repr' : 8,\n",
    "        'n_heads': 8,\n",
    "        'd_head': 64,\n",
    "    }),\n",
    ")\n",
    "\n",
    "REPR_ACT_DEC = (\n",
    "    ('space-time_attn', {\n",
    "        'n_repr' : 8,\n",
    "        'n_heads': 8,\n",
    "        'd_head': 64,\n",
    "    }),\n",
    ")\n",
    "\n",
    "class LatentAction(nn.Module):\n",
    "    '''Latent Action Model (LAM) used to distill latent actions\n",
    "    from history of past video frames. The LAM model employs a\n",
    "    VQ-VAE model to encode video frames into discrete latents.\n",
    "    Both the encoder and decoder are based on spatial-temporal\n",
    "    transformers.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_desc: Blueprint,\n",
    "        dec_desc: Blueprint,\n",
    "        d_codebook: int,\n",
    "        inp_channels: int = 3,\n",
    "        inp_shape : int | Tuple[int, int] = (64, 64),\n",
    "        ker_size : int | Tuple[int, int] = 3,\n",
    "        n_embd: int = 64,\n",
    "        n_codebook: int = 2,\n",
    "        lfq_bias : bool = True,\n",
    "        lfq_frac_sample : float = 1.,\n",
    "        lfq_commit_weight : float = 0.25,\n",
    "        lfq_entropy_weight : float = 0.1,\n",
    "        lfq_diversity_weight : float = 1.,\n",
    "        quant_loss_weight : float = 1.,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(inp_shape, int): inp_shape = (inp_shape, inp_shape)\n",
    "        \n",
    "        self.proj_in = CausalConv3d(\n",
    "            inp_channels,\n",
    "            out_channels=n_embd,\n",
    "            kernel_size=ker_size\n",
    "        )\n",
    "        \n",
    "        self.proj_out = CausalConv3d(\n",
    "            n_embd,\n",
    "            out_channels=inp_channels,\n",
    "            kernel_size=ker_size\n",
    "        )\n",
    "        \n",
    "        # Build the encoder and decoder based on the blueprint\n",
    "        self.enc_layers, self.enc_ext = parse_blueprint(enc_desc)\n",
    "        self.dec_layers, self.dec_ext = parse_blueprint(dec_desc)\n",
    "        \n",
    "        # Keep track of space-time up/down factors\n",
    "        enc_fact = prod(enc.factor for enc in self.enc_layers if isinstance(enc, (Downsample, Upsample)))\n",
    "        dec_fact = prod(dec.factor for dec in self.dec_layers if isinstance(dec, (Downsample, Upsample)))\n",
    "        \n",
    "        assert enc_fact * dec_fact == 1, 'The product of the space-time up/down factors must be 1.'\n",
    "        \n",
    "        # Add the projections to the action space\n",
    "        self.to_act = nn.Sequential(\n",
    "                Rearrange('b c t ... -> b t (c ...)'),\n",
    "                nn.Linear(\n",
    "                    int(n_embd * enc_fact * prod(inp_shape)),\n",
    "                    d_codebook,\n",
    "                    bias=False,\n",
    "                )\n",
    "        )\n",
    "\n",
    "        # Build the quantization module\n",
    "        self.quant = LookupFreeQuantization(\n",
    "            codebook_dim       = d_codebook,\n",
    "            num_codebook       = n_codebook,\n",
    "            input_dim=d_codebook * n_codebook,\n",
    "            use_bias         = lfq_bias,\n",
    "            frac_sample      = lfq_frac_sample,\n",
    "            commit_weight    = lfq_commit_weight,\n",
    "            entropy_weight   = lfq_entropy_weight,\n",
    "            diversity_weight = lfq_diversity_weight,\n",
    "        )\n",
    "        \n",
    "        self.d_codebook = d_codebook\n",
    "        self.n_codebook = n_codebook\n",
    "        self.quant_loss_weight = quant_loss_weight\n",
    "        \n",
    "    def sample(self, idxs : Tensor) -> Tensor:\n",
    "        '''Sample the action codebook values based on the indices.'''\n",
    "        return self.quant.codebook[idxs]\n",
    "        \n",
    "    def encode(\n",
    "        self,\n",
    "        video: Tensor,\n",
    "        mask : Tensor | None = None,\n",
    "        transpose : bool = False,\n",
    "    ) -> Tuple[Tuple[Tensor, Tensor], Tensor]:\n",
    "        video = self.proj_in(video)\n",
    "        \n",
    "        # Encode the video frames into latent actions\n",
    "        for enc in self.enc_layers:\n",
    "            video = enc(video, mask=mask)\n",
    "        \n",
    "        # Project to latent action space\n",
    "        act : Tensor = self.to_act(video)\n",
    "\n",
    "        # Quantize the latent actions\n",
    "        (act, idxs), q_loss = self.quant(act, transpose=transpose)\n",
    "        \n",
    "        return (act, idxs, video), q_loss\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        video : Tensor,\n",
    "        q_act : Tensor,\n",
    "    ) -> Tensor:        \n",
    "        # Decode the video frames based on past history and\n",
    "        # the quantized latent actions\n",
    "        for dec, has_ext in zip(self.dec_layers, self.dec_ext):\n",
    "            video = dec(\n",
    "                video,\n",
    "                cond=(\n",
    "                    None, # No space condition\n",
    "                    q_act if has_ext else None,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        recon = self.proj_out(video)\n",
    "        \n",
    "        return recon\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        video: Tensor,\n",
    "        mask : Tensor | None = None,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \n",
    "        # Encode the video frames into latent actions\n",
    "        (act, idxs, enc_video), q_loss = self.encode(video, mask=mask)\n",
    "        \n",
    "        # Decode the last video frame based on all the previous\n",
    "        # frames and the quantized latent actions\n",
    "        recon = self.decode(enc_video, act)\n",
    "        \n",
    "        # Compute the reconstruction loss\n",
    "        # Reconstruction loss\n",
    "        rec_loss = mse_loss(recon, video)\n",
    "        \n",
    "        # Compute the total loss by combining the individual\n",
    "        # losses, weighted by the corresponding loss weights\n",
    "        loss = rec_loss\\\n",
    "            + q_loss * self.quant_loss_weight\n",
    "        \n",
    "        return idxs, loss, (\n",
    "            rec_loss,\n",
    "            q_loss,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5288d4-88b1-45f5-9838-787adaf75372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LatentActionClassifier(nn.Module):\n",
    "    def __init__(self, latent_size: Tuple[int, int], hidden_dim: int = 128, num_classes: int = 25):\n",
    "        \"\"\"\n",
    "        MLP for processing latent representations.\n",
    "        \n",
    "        Args:\n",
    "            latent_size (Tuple[int, int]): Size of the latent tensor (T, num_codebook).\n",
    "            hidden_dim (int): Dimension of the hidden layers.\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(LatentActionClassifier, self).__init__()\n",
    "        \n",
    "        T, num_codebook = latent_size  # Extract latent dimensions\n",
    "        \n",
    "        input_dim = T * num_codebook  # Flattened input size\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Input to hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Hidden to hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)  # Hidden to output\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Latent input tensor of shape (B, T, num_codebook).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Logits of shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        # Flatten the latent tensor: (B, T, num_codebook) -> (B, T * num_codebook)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through the MLP\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276c88e3-09ef-4ed3-aea9-6021656ddd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_ACT_ENC = (\n",
    "    ('space-time_attn', {\n",
    "        'n_head': 8,\n",
    "        'd_head': 8,\n",
    "        'd_inp': 64,\n",
    "        'd_out': 64,\n",
    "    }),\n",
    "    ('spacetime_downsample', {\n",
    "        'in_channels': 64,\n",
    "        'out_channels': 64,\n",
    "        'kernel_size': 3,\n",
    "        'space_factor': 2,\n",
    "        'time_factor': 1,\n",
    "    }),\n",
    ")\n",
    "\n",
    "LATENT_ACT_DEC = (\n",
    "    ('spacetime_upsample', {\n",
    "        'in_dim': 64,\n",
    "        'out_dim': 64,\n",
    "        # 'kernel_size': 3,\n",
    "        'space_factor': 2,\n",
    "        'time_factor': 1,\n",
    "    }),\n",
    "    ('space-time_attn', {\n",
    "        'n_head': 8,\n",
    "        'd_head': 8,\n",
    "        'd_inp': 64,\n",
    "        'd_out': 64,\n",
    "    }),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e57b5547-d1d0-4e70-a567-2547ca1738c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Example Dataset and DataLoader (Replace with actual dataset)\n",
    "class ExampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = torch.randn(3, 16, 64, 64)  # Example video (C, T, H, W)\n",
    "        label = torch.randn(25) # Example action label\n",
    "        return video, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73f41ec-836a-484f-9726-ed61404927dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wm-group11-1x-challenge3/venv/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, GPUs available: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/13 [00:00<?, ?batch/s]/workspace/wm-group11-1x-challenge3/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training Epoch 1:  31%|███       | 4/13 [00:06<00:14,  1.64s/batch, mlp_loss=1.1, recon_loss=2.46, total_loss=4.15]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Update losses for progress bar and epoch tracking\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m epoch_recon_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     81\u001b[0m epoch_mlp_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mlp_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Video (B, T, C, H, W)\n",
    "# Action (B, A)\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}, GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Instantiate the LatentAction model\n",
    "latent_action_model = LatentAction(\n",
    "    enc_desc=LATENT_ACT_ENC,\n",
    "    dec_desc=LATENT_ACT_DEC,\n",
    "    d_codebook=10,\n",
    "    n_codebook=1,\n",
    "    inp_channels=3,  \n",
    "    inp_shape=(64, 64),\n",
    "    n_embd=64,\n",
    ")\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     latent_action_model = torch.nn.DataParallel(latent_action_model)\n",
    "\n",
    "latent_action_model = latent_action_model.to(device)\n",
    "\n",
    "# Instantiate the integrated classifier\n",
    "classifier = LatentActionClassifier(latent_size=[16, 10]).to(device)\n",
    "\n",
    "# Combine both models\n",
    "latent_action_model.classifier = classifier\n",
    "\n",
    "dataset = ExampleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(\n",
    "    list(latent_action_model.parameters()) + list(classifier.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "latent_action_model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    epoch_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    epoch_mlp_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\")\n",
    "    \n",
    "    for videos, labels in progress_bar:\n",
    "        # Videos: (B, C, T, H, W)\n",
    "        # Labels: (B,)\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass through the LatentAction model\n",
    "        idxs, latent_loss, (recon_loss, quant_loss) = latent_action_model(videos)\n",
    "\n",
    "        # Compute refined embeddings from the quantized indices\n",
    "        refined_embeddings = latent_action_model.sample(idxs)\n",
    "\n",
    "        # Forward pass through the classifier\n",
    "        logits = classifier(refined_embeddings)\n",
    "\n",
    "        # Classification loss\n",
    "        mlp_loss = F.mse_loss(logits, labels)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = latent_loss + mlp_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses for progress bar and epoch tracking\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_mlp_loss += mlp_loss.item()\n",
    "\n",
    "        # Update progress bar with individual losses\n",
    "        progress_bar.set_postfix(\n",
    "            total_loss=total_loss.item(),\n",
    "            recon_loss=recon_loss.item(),\n",
    "            mlp_loss=mlp_loss.item(),\n",
    "        )\n",
    "\n",
    "    # Print summary of losses at the end of the epoch\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1} completed. \"\n",
    "    #     f\"Total Loss: {epoch_loss:.4f}, \"\n",
    "    #     f\"Reconstruction Loss: {epoch_recon_loss:.4f}, \"\n",
    "    #     f\"MLP Loss: {epoch_mlp_loss:.4f}\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b0bbb-0408-4639-be74-a406d72239fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6693533b-a891-4d5b-af4f-5e2e26333faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02116361 -0.00173624 -0.13813464  0.19783622  0.02508382 -0.04306014\n",
      "  0.29541606  0.00993608 -0.02094108 -1.2919362   0.02760345  0.20123062\n",
      " -0.00949717  0.29516238 -0.00792499  0.02227568 -1.2910248  -0.02258378\n",
      "  0.19646356 -0.01153214  0.01455284  0.          0.          1.\n",
      "  0.83336455]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "data = np.load('/workspace/wm-group11-1x-challenge3/combined_action_data/frame_045376.npy')\n",
    "\n",
    "# Print or process the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01f38ad4-dc0b-41c9-91b9-1bb3d7f4193b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238.7241113339357"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10832584/45377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76c22dca-887b-40f8-8f80-eda719ae9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class CustomDataset(nn.Module):\n",
    "    def __init__(self, action_dir, video_dir):\n",
    "        super(CustomDataset, self).__init__()\n",
    "\n",
    "        action_files = [f for f in os.listdir(action_dir) if f.endswith('.npy')]\n",
    "        video_files = [f for f in os.listdir(video_dir) if f.endswith('.npy')]\n",
    "        \n",
    "        self.actions = [\n",
    "            torch.tensor(np.load(os.path.join(action_dir, action_file)), dtype=torch.float32)\n",
    "            for action_file in action_files\n",
    "        ]\n",
    "        \n",
    "        self.videos = [\n",
    "            torch.tensor(np.load(os.path.join(video_dir, video_file)), dtype=torch.float32)\n",
    "            for video_file in video_files\n",
    "        ]\n",
    "\n",
    "        print(len(self.actions))\n",
    "        print(len(self.videos))\n",
    "        # assert len(self.actions) == len(self.videos), \"Mismatch between actions and videos!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.videos[idx], self.actions[idx]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e1e956c-5512-4526-9d5d-e42cf111ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53137\n",
      "53137\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(\n",
    "    action_dir=\"/workspace/wm-group11-1x-challenge3/combined_action_data\", \n",
    "    video_dir=\"/workspace/wm-group11-1x-challenge3/combined_action_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdb415ee-2fc3-45c6-9505-9346dc605347",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dataset.actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d2d4ed4-6796-4761-9df3-8d899ac6a7ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Count: 53190\n",
      "Video Count: 53190\n"
     ]
    }
   ],
   "source": [
    "action_dir=\"/workspace/wm-group11-1x-challenge3/combined_action_data\"\n",
    "video_dir=\"/workspace/wm-group11-1x-challenge3/combined_action_data\"\n",
    "\n",
    "action_files = [f for f in os.listdir(action_dir) if f.endswith('.npy')]\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.npy')]\n",
    "\n",
    "print(\"Action Count:\", len(action_files))\n",
    "print(\"Video Count:\", len(video_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96e606-33e4-4ab7-a439-c73f5c7d564d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
