{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc010fbd-ebf4-4dfb-8c1e-1a344d44fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genie import LatentAction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3806aa-01a6-41fa-aaf4-de0ff47a9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from math import prod\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from genie.module import parse_blueprint\n",
    "from genie.module.quantization import LookupFreeQuantization\n",
    "from genie.module.video import CausalConv3d, Downsample, Upsample\n",
    "from genie.utils import Blueprint\n",
    "\n",
    "REPR_ACT_ENC = (\n",
    "    ('space-time_attn', {\n",
    "        'n_repr' : 8,\n",
    "        'n_heads': 8,\n",
    "        'd_head': 64,\n",
    "    }),\n",
    ")\n",
    "\n",
    "REPR_ACT_DEC = (\n",
    "    ('space-time_attn', {\n",
    "        'n_repr' : 8,\n",
    "        'n_heads': 8,\n",
    "        'd_head': 64,\n",
    "    }),\n",
    ")\n",
    "\n",
    "class LatentAction(nn.Module):\n",
    "    '''Latent Action Model (LAM) used to distill latent actions\n",
    "    from history of past video frames. The LAM model employs a\n",
    "    VQ-VAE model to encode video frames into discrete latents.\n",
    "    Both the encoder and decoder are based on spatial-temporal\n",
    "    transformers.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_desc: Blueprint,\n",
    "        dec_desc: Blueprint,\n",
    "        d_codebook: int,\n",
    "        inp_channels: int = 3,\n",
    "        inp_shape : int | Tuple[int, int] = (64, 64),\n",
    "        ker_size : int | Tuple[int, int] = 3,\n",
    "        n_embd: int = 64,\n",
    "        n_codebook: int = 2,\n",
    "        lfq_bias : bool = True,\n",
    "        lfq_frac_sample : float = 1.,\n",
    "        lfq_commit_weight : float = 0.25,\n",
    "        lfq_entropy_weight : float = 0.1,\n",
    "        lfq_diversity_weight : float = 1.,\n",
    "        quant_loss_weight : float = 1.,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(inp_shape, int): inp_shape = (inp_shape, inp_shape)\n",
    "        \n",
    "        self.proj_in = CausalConv3d(\n",
    "            inp_channels,\n",
    "            out_channels=n_embd,\n",
    "            kernel_size=ker_size\n",
    "        )\n",
    "        \n",
    "        self.proj_out = CausalConv3d(\n",
    "            n_embd,\n",
    "            out_channels=inp_channels,\n",
    "            kernel_size=ker_size\n",
    "        )\n",
    "        \n",
    "        # Build the encoder and decoder based on the blueprint\n",
    "        self.enc_layers, self.enc_ext = parse_blueprint(enc_desc)\n",
    "        self.dec_layers, self.dec_ext = parse_blueprint(dec_desc)\n",
    "        \n",
    "        # Keep track of space-time up/down factors\n",
    "        enc_fact = prod(enc.factor for enc in self.enc_layers if isinstance(enc, (Downsample, Upsample)))\n",
    "        dec_fact = prod(dec.factor for dec in self.dec_layers if isinstance(dec, (Downsample, Upsample)))\n",
    "        \n",
    "        assert enc_fact * dec_fact == 1, 'The product of the space-time up/down factors must be 1.'\n",
    "        \n",
    "        # Add the projections to the action space\n",
    "        self.to_act = nn.Sequential(\n",
    "                Rearrange('b c t ... -> b t (c ...)'),\n",
    "                nn.Linear(\n",
    "                    int(n_embd * enc_fact * prod(inp_shape)),\n",
    "                    d_codebook,\n",
    "                    bias=False,\n",
    "                )\n",
    "        )\n",
    "\n",
    "        # Build the quantization module\n",
    "        self.quant = LookupFreeQuantization(\n",
    "            codebook_dim       = d_codebook,\n",
    "            num_codebook       = n_codebook,\n",
    "            input_dim=d_codebook * n_codebook,\n",
    "            use_bias         = lfq_bias,\n",
    "            frac_sample      = lfq_frac_sample,\n",
    "            commit_weight    = lfq_commit_weight,\n",
    "            entropy_weight   = lfq_entropy_weight,\n",
    "            diversity_weight = lfq_diversity_weight,\n",
    "        )\n",
    "        \n",
    "        self.d_codebook = d_codebook\n",
    "        self.n_codebook = n_codebook\n",
    "        self.quant_loss_weight = quant_loss_weight\n",
    "        \n",
    "    def sample(self, idxs : Tensor) -> Tensor:\n",
    "        '''Sample the action codebook values based on the indices.'''\n",
    "        return self.quant.codebook[idxs]\n",
    "        \n",
    "    def encode(\n",
    "        self,\n",
    "        video: Tensor,\n",
    "        mask : Tensor | None = None,\n",
    "        transpose : bool = False,\n",
    "    ) -> Tuple[Tuple[Tensor, Tensor], Tensor]:\n",
    "        video = self.proj_in(video)\n",
    "        \n",
    "        # Encode the video frames into latent actions\n",
    "        for enc in self.enc_layers:\n",
    "            video = enc(video, mask=mask)\n",
    "        \n",
    "        # Project to latent action space\n",
    "        act : Tensor = self.to_act(video)\n",
    "\n",
    "        # Quantize the latent actions\n",
    "        (act, idxs), q_loss = self.quant(act, transpose=transpose)\n",
    "        \n",
    "        return (act, idxs, video), q_loss\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        video : Tensor,\n",
    "        q_act : Tensor,\n",
    "    ) -> Tensor:        \n",
    "        # Decode the video frames based on past history and\n",
    "        # the quantized latent actions\n",
    "        for dec, has_ext in zip(self.dec_layers, self.dec_ext):\n",
    "            video = dec(\n",
    "                video,\n",
    "                cond=(\n",
    "                    None, # No space condition\n",
    "                    q_act if has_ext else None,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        recon = self.proj_out(video)\n",
    "        \n",
    "        return recon\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        video: Tensor,\n",
    "        mask : Tensor | None = None,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \n",
    "        # Encode the video frames into latent actions\n",
    "        (act, idxs, enc_video), q_loss = self.encode(video, mask=mask)\n",
    "        \n",
    "        # Decode the last video frame based on all the previous\n",
    "        # frames and the quantized latent actions\n",
    "        recon = self.decode(enc_video, act)\n",
    "        \n",
    "        # Compute the reconstruction loss\n",
    "        # Reconstruction loss\n",
    "        rec_loss = mse_loss(recon, video)\n",
    "        \n",
    "        # Compute the total loss by combining the individual\n",
    "        # losses, weighted by the corresponding loss weights\n",
    "        loss = rec_loss\\\n",
    "            + q_loss * self.quant_loss_weight\n",
    "        \n",
    "        return idxs, loss, (\n",
    "            rec_loss,\n",
    "            q_loss,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5288d4-88b1-45f5-9838-787adaf75372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LatentActionClassifier(nn.Module):\n",
    "    def __init__(self, latent_size: Tuple[int, int], hidden_dim: int = 128, num_classes: int = 25):\n",
    "        \"\"\"\n",
    "        MLP for processing latent representations.\n",
    "        \n",
    "        Args:\n",
    "            latent_size (Tuple[int, int]): Size of the latent tensor (T, num_codebook).\n",
    "            hidden_dim (int): Dimension of the hidden layers.\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(LatentActionClassifier, self).__init__()\n",
    "        \n",
    "        T, num_codebook = latent_size  # Extract latent dimensions\n",
    "        \n",
    "        input_dim = T * num_codebook  # Flattened input size\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Input to hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Hidden to hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)  # Hidden to output\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Latent input tensor of shape (B, T, num_codebook).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Logits of shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        # Flatten the latent tensor: (B, T, num_codebook) -> (B, T * num_codebook)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through the MLP\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c88e3-09ef-4ed3-aea9-6021656ddd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_ACT_ENC = (\n",
    "    ('space-time_attn', {\n",
    "        'n_head': 8,\n",
    "        'd_head': 8,\n",
    "        'd_inp': 64,\n",
    "        'd_out': 64,\n",
    "    }),\n",
    "    ('spacetime_downsample', {\n",
    "        'in_channels': 64,\n",
    "        'out_channels': 64,\n",
    "        'kernel_size': 3,\n",
    "        'space_factor': 2,\n",
    "        'time_factor': 1,\n",
    "    }),\n",
    ")\n",
    "\n",
    "LATENT_ACT_DEC = (\n",
    "    ('spacetime_upsample', {\n",
    "        'in_dim': 64,\n",
    "        'out_dim': 64,\n",
    "        # 'kernel_size': 3,\n",
    "        'space_factor': 2,\n",
    "        'time_factor': 1,\n",
    "    }),\n",
    "    ('space-time_attn', {\n",
    "        'n_head': 8,\n",
    "        'd_head': 8,\n",
    "        'd_inp': 64,\n",
    "        'd_out': 64,\n",
    "    }),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b5547-d1d0-4e70-a567-2547ca1738c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Example Dataset and DataLoader (Replace with actual dataset)\n",
    "class ExampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = torch.randn(3, 16, 64, 64)  # Example video (C, T, H, W)\n",
    "        label = torch.randn(25) # Example action label\n",
    "        return video, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f41ec-836a-484f-9726-ed61404927dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video (B, T, C, H, W)\n",
    "# Action (B, A)\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}, GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Instantiate the LatentAction model\n",
    "latent_action_model = LatentAction(\n",
    "    enc_desc=LATENT_ACT_ENC,\n",
    "    dec_desc=LATENT_ACT_DEC,\n",
    "    d_codebook=10,\n",
    "    n_codebook=1,\n",
    "    inp_channels=3,  \n",
    "    inp_shape=(64, 64),\n",
    "    n_embd=64,\n",
    ")\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     latent_action_model = torch.nn.DataParallel(latent_action_model)\n",
    "\n",
    "latent_action_model = latent_action_model.to(device)\n",
    "\n",
    "# Instantiate the integrated classifier\n",
    "classifier = LatentActionClassifier(latent_size=[16, 10]).to(device)\n",
    "\n",
    "# Combine both models\n",
    "latent_action_model.classifier = classifier\n",
    "\n",
    "dataset = ExampleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(\n",
    "    list(latent_action_model.parameters()) + list(classifier.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "latent_action_model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    epoch_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    epoch_mlp_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\")\n",
    "    \n",
    "    for videos, labels in progress_bar:\n",
    "        # Videos: (B, C, T, H, W)\n",
    "        # Labels: (B,)\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass through the LatentAction model\n",
    "        idxs, latent_loss, (recon_loss, quant_loss) = latent_action_model(videos)\n",
    "\n",
    "        # Compute refined embeddings from the quantized indices\n",
    "        refined_embeddings = latent_action_model.sample(idxs)\n",
    "\n",
    "        # Forward pass through the classifier\n",
    "        logits = classifier(refined_embeddings)\n",
    "\n",
    "        # Classification loss\n",
    "        mlp_loss = F.mse_loss(logits, labels)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = latent_loss + mlp_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses for progress bar and epoch tracking\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_mlp_loss += mlp_loss.item()\n",
    "\n",
    "        # Update progress bar with individual losses\n",
    "        progress_bar.set_postfix(\n",
    "            total_loss=total_loss.item(),\n",
    "            recon_loss=recon_loss.item(),\n",
    "            mlp_loss=mlp_loss.item(),\n",
    "        )\n",
    "\n",
    "    # Print summary of losses at the end of the epoch\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1} completed. \"\n",
    "    #     f\"Total Loss: {epoch_loss:.4f}, \"\n",
    "    #     f\"Reconstruction Loss: {epoch_recon_loss:.4f}, \"\n",
    "    #     f\"MLP Loss: {epoch_mlp_loss:.4f}\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b0bbb-0408-4639-be74-a406d72239fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (open_genie_venv)",
   "language": "python",
   "name": "open_genie_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
